{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ae24b8c-6e5c-4be8-9f21-8e74dc61f0b2",
   "metadata": {},
   "source": [
    "# Data Pre-Processing Phase 2 - Deriving new relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9ae4378-1491-420a-8bc7-9a7d9bb7183c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from collections import Counter, defaultdict, deque\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "import tracemalloc\n",
    "from tqdm import tqdm\n",
    "import pytz  # type: ignore\n",
    "from tweepy import User\n",
    "from shutil import copy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c91d244-33c0-4952-ad8d-6832e5865594",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input data\n",
    "DATA_DIR = Path('/data/TwiBot-22')\n",
    "USER_DIR = DATA_DIR / 'processed' / 'users'\n",
    "RELATION_DIR = DATA_DIR / 'processed' / 'relations'\n",
    "\n",
    "# Output directory\n",
    "OUT_DIR = DATA_DIR / 'processed' / 'derived'\n",
    "OUT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "137fd3db-5182-4be5-95b2-c6eb82e1c3ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "users = [user for user in USER_DIR.glob(\"*\")]\n",
    "usernames = set([user.stem for user in users])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900c79a6-a33b-4ba4-a235-231a56ec4b63",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Retweeted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692cf0f1-743e-481f-8d6b-14499d85f765",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "We already have the `retweeted` relation, however, we need to transform it from tweet entities to user entities.\n",
    "We start out with a mapping of the following form:\n",
    "\n",
    "| source_id | relation | target_id |\n",
    "|----------------------|-----------|---------------------|\n",
    "| t1250366502454296576 | retweeted | t1250345547874979842\n",
    "| t1488734114794532870 | retweeted | t1488722567741464579\n",
    "| t1431316326992146438 | retweeted | t1430918145397645315\n",
    "\n",
    "Then we merge this DataFrame twice, we use the `post` relation to get the users for the corresponding tweets.\n",
    "\n",
    "| source_id | relation | target_id |\n",
    "|------------|------|--------------------|\n",
    "| u124137240 | post | t1497615862374998018 |\n",
    "| u56901370 | post | t1487344639556210689 |\n",
    "| u2943060805 | post | t1498507430187122688 |\n",
    "\n",
    "\n",
    "As a result, we get two DataFrames formatted like this:\n",
    "\n",
    "| source_id_user | source_id_tweet | target_id_tweet |\n",
    "| ---------------------|----------------------|----------------------|\n",
    "| u1076092249279217664 | t1250366502454296576 | t1250345547874979842 |\n",
    "| u384126992 | t1488734114794532870 | t1488722567741464579 |\n",
    "| u76896397 | t1431316326992146438 | t1430918145397645315 |\n",
    "\n",
    "\n",
    "| source_id_tweet | target_id_tweet | target_id_user |\n",
    "|----------------------|----------------------|----------------------|\n",
    "| t1250366502454296576 | t1250345547874979842 | u1207540481997099009 |\n",
    "| t1488734114794532870 | t1488722567741464579 | u933480950515863552 |\n",
    "| t1488865835636641795 | t1488722567741464579 | u933480950515863552 |\n",
    "\n",
    "Finally, we need to merge them and discard all the unnecessary columns.\n",
    "\n",
    "| source_id   | relation  | target_id  |\n",
    "|-------------|-----------|------------|\n",
    "| u52664889   | retweeted_user | u868112707 |\n",
    "| u337483640  | retweeted_user | u4795561   |\n",
    "| u3301643341 | retweeted_user | u691353    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9238d975-6103-4a26-aade-ae7a6a28b63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retweeted_df = pd.read_csv(RELATION_DIR / 'retweeted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de810b8a-c10f-4448-a0c1-38f629ee59a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df = pd.read_csv(RELATION_DIR / 'post.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85603c1a-b03b-49c7-83d9-8efd5c80e570",
   "metadata": {},
   "source": [
    "### Retweeted Merge user for Source_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f89bbd43-6779-4805-b27a-8f404e2764c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "retweeted_user_left = pd.merge(retweeted_df[[\"source_id\", \"target_id\"]], post_df[[\"source_id\", \"target_id\"]], left_on='source_id', right_on='target_id', suffixes=('_retweeted', '_post'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "537fec36-2873-4e94-907e-5e4156543439",
   "metadata": {},
   "outputs": [],
   "source": [
    "retweeted_user_left = retweeted_user_left[[\"source_id_post\", \"source_id_retweeted\", \"target_id_retweeted\"]]\n",
    "retweeted_user_left.columns = [\"source_id_user\", \"source_id_tweet\", \"target_id_tweet\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa9cb0f-4609-4ea0-a018-3543f004478d",
   "metadata": {},
   "source": [
    "Lets make a quick sanity check that all our operation had the intended effect.\n",
    "We want to make sure that all values are mapped correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b1bc509-490c-4a7a-a087-54fdde8b8a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert post_df.loc[post_df[\"target_id\"] == retweeted_user_left.iloc[0][\"source_id_tweet\"]][\"source_id\"].item() == retweeted_user_left.iloc[0][\"source_id_user\"], \"wrong mapping\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ff40f2a-986d-4534-bdc7-9d56babe8840",
   "metadata": {},
   "outputs": [],
   "source": [
    "retweeted_user_left = retweeted_user_left.sort_values(by=[\"source_id_tweet\", \"target_id_tweet\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12e2f55-125f-4d50-a73c-62e617dad86d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Retweeted Merge user for target_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38422eff-2628-4ee4-b083-46e6cbe99141",
   "metadata": {},
   "outputs": [],
   "source": [
    "retweeted_user_right = pd.merge(retweeted_df[[\"source_id\", \"target_id\"]], post_df[[\"source_id\", \"target_id\"]], left_on='target_id', right_on='target_id', suffixes=('_retweeted', '_post'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42de81ba-cfac-4a92-a7bf-9c6d7cc3da22",
   "metadata": {},
   "outputs": [],
   "source": [
    "retweeted_user_right.columns = [\"source_id_tweet\", \"target_id_tweet\", \"target_id_user\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15432fb1-e144-4807-8203-fdaecab74b4b",
   "metadata": {},
   "source": [
    "Sanity checks again, this time we also check that the shape of both dataframes is the same before we merge them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2484b5d-ffac-4cbd-9c98-77b413f69d31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert post_df.loc[post_df[\"target_id\"] == retweeted_user_right.iloc[0][\"target_id_tweet\"]][\"source_id\"].item() == retweeted_user_right.iloc[0][\"target_id_user\"], \"Wrong mapping\"\n",
    "assert retweeted_user_left.shape == retweeted_user_right.shape, \"Shapes do not match!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b983bf0-3a91-4985-9b1f-b94781524791",
   "metadata": {},
   "outputs": [],
   "source": [
    "retweeted_user = pd.merge(retweeted_user_left, retweeted_user_right, left_on=[\"source_id_tweet\", \"target_id_tweet\"], right_on=[\"source_id_tweet\", \"target_id_tweet\"]) #, suffixes=('_left', '_right'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a802e5d-ac69-4238-a77c-cc4ee0cb98c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retweeted_user[\"relation\"] = \"retweeted_user\"\n",
    "retweeted_user = retweeted_user[[\"source_id_user\", \"relation\", \"target_id_user\"]]\n",
    "retweeted_user.columns = [\"source_id\", \"relation\", \"target_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0655d277-4eb1-491a-9175-4b212a591542",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert retweeted_user.shape == retweeted_user_right.shape, \"shape mismatch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6de8cc8f-e4be-48ac-9b4b-beb986be6ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retweeted_user.to_csv(OUT_DIR / \"retweeted_user.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4027f36-4297-4045-a0c5-59935a2e8018",
   "metadata": {},
   "source": [
    "## New relation: Co-Retweeted\n",
    "Now we aim to find users that retweeted the same tweet.\n",
    "We reuse the `retweeted_user_left` DataFrame. So, we start out with:\n",
    "\n",
    "\n",
    "| source_id_user | source_id_tweet | target_id_tweet |\n",
    "| ---------------------|----------------------|----------------------|\n",
    "| u1076092249279217664 | t1250366502454296576 | t1250345547874979842 |\n",
    "| u384126992 | t1488734114794532870 | t1488722567741464579 |\n",
    "| u76896397 | t1431316326992146438 | t1430918145397645315 |\n",
    "\n",
    "Then, we group this DataFrame by the `target_id_tweet` column and discard the `source_id_tweet` column.\n",
    "Now we have a list of users that retweeted a tweet for each entry in `target_id_tweet`.\n",
    "For this relation we are only interested in tweets that get retweeted by more than one user, therefore, we discard all rows where the resulting list does not contain at least 2 users.\n",
    "We end up with a DataFrame that looks this:\n",
    "\n",
    "| target_id_tweet | source_id_user |\n",
    "|----------------------|------------------------------------|\n",
    "| t1000124286948990976 | [u2196348115, u1281526188] |\n",
    "| t1000207766819098624 | [u712263962, u243036743] |\n",
    "| t1000346560905887745 | [u2981968107, u15960453, u1150918] |\n",
    "\n",
    "Finally, we take the lists inside `source_id_user` and create combinations of all users, excl. pairs with the same user twice.\n",
    "These are now our edges.\n",
    "\n",
    "| source_id | relation | target_id |\n",
    "|---------------------|--------------|-------------|\n",
    "| u28649627 | co_retweeted | u2462436602 |\n",
    "| u243292912 | co_retweeted | u1700157950 |\n",
    "| u960015338095239168 | co_retweeted | u1400517288 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94f99bef-d4ec-4728-a3e7-d44e02f2779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retweeted_grouped = retweeted_user_left.groupby(\"target_id_tweet\")[\"source_id_user\"].apply(list).reset_index()\n",
    "retweeted_grouped = retweeted_grouped[retweeted_grouped['source_id_user'].apply(len) > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af2a34e-c5c9-4984-8fd8-cc01885692ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "We now create a list of unique permutations. We do not want users to have a relation to themselves, therefore we restrict it such that only pairs remain where the first and second element differ,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d26ae333-46dd-4b08-8948-53c05ff24266",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = set([(a1, a2) for lst in retweeted_grouped['source_id_user'] for a1, a2 in itertools.permutations(lst, 2) if a1 != a2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0740add2-c1a5-4642-be0e-086da9115eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_retweeted_df = pd.DataFrame(pairs, columns=[\"source_id\", \"target_id\"])\n",
    "co_retweeted_df[\"relation\"] = \"co_retweeted\"\n",
    "co_retweeted_df = co_retweeted_df[[\"source_id\", \"relation\", \"target_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2428f0a-441c-4d8b-8dff-5430e045084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_retweeted_df.to_csv(OUT_DIR / \"co_retweeted.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e009ffa0-63ed-42da-8507-9cd3dcbce0d2",
   "metadata": {},
   "source": [
    "# Co-Hashtag\n",
    "\n",
    "To derive the *co-hashtag* relation we can follow a similar approach to *co-retweet*. We start out with:\n",
    "\n",
    "| source_id | relation | target_id |\n",
    "|----------------------|---------|----------|\n",
    "| t1146699924525977611 | discuss | h4051 |\n",
    "| t1296249717257625602 | discuss | h396644 |\n",
    "| t1497194047244730369 | discuss | h5108039 |\n",
    "\n",
    "And apply some transformation steps such that we end up with a table like this:\n",
    "\n",
    "| source_id | relation | target_id |\n",
    "|---------------------|---------|-------|\n",
    "| u927875932093927424 | discuss | h4051 |\n",
    "| u927875932093927424 | discuss | h25078 |\n",
    "| u927875932093927424 | discuss | h2866 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c14f455-c0aa-4fa1-8ad3-8234e77e9b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66000633, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discuss_df = pd.read_csv(RELATION_DIR / 'discuss.csv')\n",
    "discuss_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84f4aa64-ad4b-455f-9f29-2b597fb7d9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5146289, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "by_hashtag = discuss_df.groupby(\"target_id\")[\"source_id\"].apply(list).reset_index()\n",
    "by_hashtag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a419fe04-c02d-46c4-aebb-80a3537d27c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66000633, 4)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "by_hashtag_user = pd.merge(discuss_df[[\"source_id\", \"target_id\"]], post_df[[\"source_id\", \"target_id\"]], left_on='source_id', right_on='target_id', suffixes=('_discuss', '_post'))\n",
    "by_hashtag_user.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "172a224e-4264-4434-b28a-1e312d1b580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_hashtag_user = by_hashtag_user[[\"target_id_discuss\", \"source_id_post\"]]\n",
    "by_hashtag_user[\"relation\"] = \"discuss\"\n",
    "by_hashtag_user = by_hashtag_user[[\"source_id_post\", \"relation\", \"target_id_discuss\"]]\n",
    "by_hashtag_user.columns = [\"source_id\", \"relation\", \"target_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b4e02a-55e2-4a46-8ece-a3c60c5d40ce",
   "metadata": {},
   "source": [
    "Drop duplicates, avoids the id twice inside `source_id` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c963f3f-c321-4a22-857c-3a6511f91753",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_hashtag_user = by_hashtag_user.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6153c12-93cb-4ec8-b1dd-ce3f3a71a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_grouped = by_hashtag_user.groupby(\"target_id\")[\"source_id\"].apply(list).reset_index()\n",
    "hashtags_grouped = hashtags_grouped[hashtags_grouped['source_id'].apply(len) > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eab417f-aa9b-4e9b-8a06-2e8955bb3df2",
   "metadata": {},
   "source": [
    "Now we end up with a table like\n",
    "\n",
    "| target_id |                                         source_id |\n",
    "|-----------|---------------------------------------------------|\n",
    "|        h0 | [u3159148494, u899737456408317952, u9083222, u... |\n",
    "|        h1 | [u1058015808, u84757176, u1474033745707487239,... |\n",
    "|      h100 | [u746639450, u722117494899679232, u12330805677... |\n",
    "\n",
    "However, this has two problems, first of all some hashtags are overrepresentive, as we can see in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "977823c2-f00e-47be-a0d3-a81cad32f4b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1570584.00\n",
       "mean          14.58\n",
       "std          152.62\n",
       "min            2.00\n",
       "25%            2.00\n",
       "50%            3.00\n",
       "75%            7.00\n",
       "max        82690.00\n",
       "Name: source_id, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtag_count = hashtags_grouped['source_id'].map(len)\n",
    "hashtag_count.describe().map(lambda x: format(x, '.2f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8d1adc-8560-4335-bdba-4ac1295c06b9",
   "metadata": {},
   "source": [
    "As a consequence, we apply a filtering step by a threshold before we apply the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd5abf6c-b4f9-497f-b536-f62f6ce03457",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_grouped['amount'] = hashtags_grouped['source_id'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b4b424c-52fe-4a51-98df-f8f548b36791",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERCENT = .71  # THRESHOLD = 6.0\n",
    "THRESHOLD = hashtags_grouped[\"amount\"].quantile(PERCENT)\n",
    "threshold_str = str(int(100*PERCENT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05e7e0b1-5058-4508-8e2a-4fe584c7e633",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = hashtags_grouped['amount'] <= THRESHOLD\n",
    "hashtags_grouped_keep = hashtags_grouped[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3404e5-e670-4596-b1f5-e4b421280bfd",
   "metadata": {},
   "source": [
    "However, since the amount of hashtags we have is almost two orders of magnitude bigger than the amount of retweets, we can't just calculate all permutations as above.\n",
    "Instead, we divide the DataFrame into smaller chunks, which we write to the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98be72ea-9365-479a-9786-6fdc148de91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 11711/11711 [00:45<00:00, 256.71it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "for i in tqdm(range(math.ceil(hashtags_grouped_keep.shape[0] / batch_size))):\n",
    "    source_ids = hashtags_grouped_keep.iloc[i*batch_size: (i+1)*batch_size]['source_id']\n",
    "\n",
    "    co_hashtags = defaultdict(list)\n",
    "    for row in source_ids:\n",
    "        row_comb = [(a1, a2) for a1, a2 in itertools.permutations(row, 2)]\n",
    "        [co_hashtags[a1].append(a2) for a1, a2 in row_comb]\n",
    "\n",
    "    for user, matches in co_hashtags.items():\n",
    "        filepath = OUT_DIR / 'tmp' / 'users' / user / f\"co_hashtag_{threshold_str}.txt\"\n",
    "        filepath.parent.mkdir(exist_ok=True, parents=True)\n",
    "        with open(filepath, 'a') as fh:\n",
    "            fh.write('\\n'.join(matches) + '\\n')  # important + '\\n'!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3a32ed-e05a-4da8-b616-e2601277cd97",
   "metadata": {},
   "source": [
    "Once all chunks are writte, we read `co_hashtag_*.txt` files into sub-DataFrames, which get then concatenated to one big DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83b2bd00-6d02-4312-9a5e-6be3e5246f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [05:03<00:00, 32.90it/s]\n"
     ]
    }
   ],
   "source": [
    "co_hashtag_df = pd.DataFrame([], columns=[\"source_id\", \"relation\", \"target_id\", \"amount\"])\n",
    "\n",
    "keys, vals = set(), set()\n",
    "batch_size = 100\n",
    "for i in tqdm(range(math.ceil(len(users) / batch_size))):    \n",
    "    dfs = []\n",
    "    for user in users[i*batch_size:(i+1)*batch_size]:\n",
    "        filepath = OUT_DIR / 'tmp' / 'users' / user.name / f\"co_hashtag_{threshold_str}.txt\"\n",
    "        if not filepath.is_file(): continue\n",
    "\n",
    "        keys.add(user.name)\n",
    "        \n",
    "        with open(filepath, \"r\") as fh:\n",
    "            target_ids = [user.rstrip() for user in fh.readlines()]\n",
    "\n",
    "        for tid in target_ids:\n",
    "            vals.add(tid)\n",
    "        \n",
    "        counter = Counter(target_ids)\n",
    "        counter_df = pd.DataFrame(counter.items(), columns=[\"target_id\", \"amount\"])\n",
    "        counter_df[\"source_id\"] = user.stem\n",
    "        counter_df[\"relation\"] = \"co_hashtag\"\n",
    "        counter_df = counter_df[[\"source_id\", \"relation\", \"target_id\", \"amount\"]]\n",
    "        dfs.append(counter_df)\n",
    "    X = pd.concat(dfs)\n",
    "    partial_file = OUT_DIR / 'tmp' / 'partials' / threshold_str / f'cohashtag_partial_{threshold_str}_{i}.csv'\n",
    "    partial_file.parent.mkdir(exist_ok=True, parents=True)\n",
    "    X.to_csv(partial_file, index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771f8b25-2e05-44af-90e8-8769d2ef0bb3",
   "metadata": {},
   "source": [
    "Our final table is of this format:\n",
    "\n",
    "| source_id | relation | target_id | amount |\n",
    "|----------------------|------------|---------------------|---|\n",
    "| u1470092268518395908 | co_hashtag | u3406899801 | 2 |\n",
    "| u1113469923253211136 | co_hashtag | u793501498244292609 | 3 |\n",
    "| u1113469923253211136 | co_hashtag | u314278227 | 2 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a27989b9-6fc3-492c-b577-9d879a43591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "partials = [file for file in (OUT_DIR / 'tmp' / 'partials' / threshold_str).rglob(f\"*{threshold_str}_*.csv\")]\n",
    "partials_df = [pd.read_csv(f) for f in partials]\n",
    "co_hashtag = pd.concat(partials_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ca0a19c-bc85-4be8-ae8e-3420d644204b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6954818, 4)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co_hashtag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d030e38b-c8d4-41a2-8704-afd3aba3a6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_hashtag.to_csv(OUT_DIR / f\"cohashtag_{threshold_str}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f032b9ac-f9bc-4681-ba89-2f5410e507a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Finalizing the csv for training\n",
    "\n",
    "For our project we restrict ourselves to five types of relations:\n",
    "- followers\n",
    "- following\n",
    "- retweeted_user\n",
    "- co_retweeted\n",
    "- co_hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc640a59-60ee-43c9-beb8-91086060f84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "followers = pd.read_csv(RELATION_DIR / \"followers.csv\")\n",
    "following = pd.read_csv(RELATION_DIR / \"following.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "02083e67-ce7f-4c12-826b-2a791eb85d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12530639, 4)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = pd.concat([followers, following, retweeted_user, co_retweeted_df, co_hashtag])\n",
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa3f13b4-1caf-4597-a8c9-e00d143b92ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv(OUT_DIR / \"combined\" / \"all_5rel.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
