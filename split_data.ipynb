{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ae24b8c-6e5c-4be8-9f21-8e74dc61f0b2",
   "metadata": {},
   "source": [
    "# Data Pre-Processing Phase 1 - Making the data accessible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d970cd-061c-4f17-a5b9-a1dccca95b9b",
   "metadata": {},
   "source": [
    "The TwiBot-22 dataset consists of over 100GB of textual data.\n",
    "It is necessary to preprocess and organise the data in way that will set the foundation for further analysis.\n",
    "\n",
    "In its original form the dataset is structured as follows:\n",
    "```\n",
    "TwiBot22-data\n",
    " └── original\n",
    "     ├── edge.csv       # 6.2GB\n",
    "     ├── hashtag.json   # 255MB\n",
    "     ├── label.csv      # 21MB\n",
    "     ├── list.json      # 4.7MB\n",
    "     ├── readme.md      # 1.9KB\n",
    "     ├── split.csv      # 20MB\n",
    "     ├── tweet_0.json   # 12GB\n",
    "     ├── tweet_0.json   # 12GB\n",
    "     ├── tweet_0.json   # 11GB\n",
    "     ├── tweet_1.json   # 11GB\n",
    "     ├── tweet_2.json   # 11GB\n",
    "     ├── tweet_3.json   # 11GB\n",
    "     ├── tweet_4.json   # 11GB\n",
    "     ├── tweet_5.json   # 12GB\n",
    "     ├── tweet_6.json   # 11GB\n",
    "     ├── tweet_7.json   # 12GB\n",
    "     ├── tweet_8.json   # 9.4GB\n",
    "     └── user.json      # 747MB\n",
    "```\n",
    "\n",
    "Our first phase of preprocessing includes:\n",
    "- Split `tweet_[0-9].json` into individual `tweets.json` per user\n",
    "- Split `user.json` into individual `user.json` per user\n",
    "\n",
    "Function calls in this notebook can take a significant amount of time to complete.\n",
    "Therefore cells that trigger the execution are commented out.\n",
    "To actually run these steps just uncomment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9ae4378-1491-420a-8bc7-9a7d9bb7183c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b54f956-c321-45e1-bd9e-36887861c04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('/data/TwiBot-22')\n",
    "USER_DIR = DATA_DIR / 'processed' / 'users'\n",
    "USER_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5ae694b-7237-4bb4-8d9e-40cd310165fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(filename):\n",
    "    with open(filename) as fh:\n",
    "        data = json.load(fh)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f558f5-ef2e-4e12-b576-f108d842d5f1",
   "metadata": {},
   "source": [
    "## Step 1: Split `tweets_[0-8].json` into one file per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "005a66db-58c7-465d-93f2-fc31c2f8e8eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TWEET_FILES = sorted([fh for fh in (DATA_DIR / 'original').glob(\"tweet_[0-8].json\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19b6ca6e-b4b0-46bc-a582-523b7dea6f29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_tweets_per_user(tweet_json):\n",
    "    tweets_per_user = defaultdict(list)\n",
    "    tweet_data = load_json(tweet_json)\n",
    "    for tweet in tweet_data:\n",
    "        tweets_per_user[f'u{tweet[\"author_id\"]}'].append(tweet)\n",
    "    return tweets_per_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6241b23-e039-4f30-8320-ee0baf80e23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(tweets):\n",
    "    ids = [tw[\"id\"] for tw in tweets]\n",
    "    if len(ids) != len(set(ids)):\n",
    "        new_tweets = {}\n",
    "        for tw in tweets:\n",
    "            id = tw['id']\n",
    "            if id in new_tweets:\n",
    "                assert tw == new_tweets[id]\n",
    "            else:\n",
    "                new_tweets[id] = tw\n",
    "        tweets = list(new_tweets.values())\n",
    "    return tweets       \n",
    "\n",
    "def check_for_duplicates(tweets):\n",
    "    ids = [tw[\"id\"] for tw in tweets]\n",
    "    assert len(ids) == len(set(ids)), \"Duplicates found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd1642e9-58f5-4603-8912-200eabd6e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_tweets_per_user(tweet_files):\n",
    "    for jf in tqdm(tweet_files):\n",
    "        tweets_per_user = extract_tweets_per_user(jf)\n",
    "        for user, tweets in tweets_per_user.items():\n",
    "            user_directory = USER_DIR / user\n",
    "            user_directory.mkdir(exist_ok=True)\n",
    "            jf = user_directory / 'tweets.json'\n",
    "            if jf.is_file():\n",
    "                with open(jf, \"r\") as fh:\n",
    "                    tweets = tweets + json.load(fh)\n",
    "                tweets = remove_duplicates(tweets)\n",
    "                check_for_duplicates(tweets)  # assert that no tweet is added twice\n",
    "            with open(jf, \"w\") as fh:\n",
    "                json.dump(tweets, fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc4adca-30e5-4a69-bad5-9862e3cb069c",
   "metadata": {},
   "source": [
    "This snippet will take a few hours to complete and its only necessary to run it once, since the results will be saved afterwards.<br>\n",
    "However, one can split the big `tweets_[0-8].json` filtes into a `tweet.json` per user by running the following loop.\n",
    "\n",
    "Most likely the kernel will die at some point should this script be run for all 8 json in a loop. Therefore, we recommend to run it manually 8 times.\n",
    "\n",
    "\n",
    "```python\n",
    "    export_tweets_per_user([TWEET_FILES[i]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed104e6c-a0cb-41f0-8b76-f4f32d52b0e5",
   "metadata": {},
   "source": [
    "After this step is completed, there should be 933.872 user-specific directories in `USER_DIR`<br>\n",
    "$\\Longrightarrow$ $\\approx$ 93% of users tweeted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c752e2be-0a47-40c3-acb8-2c781f695e2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2: Split `user.json` into one file per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "088f44e3-cd10-4e87-9644-29d7dcde8fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_FILE = DATA_DIR / 'original' / 'user.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e23fda1d-fb94-4b21-b34e-7a53e45327aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_json_into_subfiles(json_file, destination):\n",
    "    data = load_json(json_file)\n",
    "    for entry in tqdm(data):\n",
    "        current_entry = USER_DIR / entry[\"id\"]\n",
    "        if not current_entry.is_dir():\n",
    "            current_entry.mkdir(exist_ok=True)\n",
    "        with open(current_entry / json_file.name, \"w\") as fh:\n",
    "            json.dump(entry, fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57eaf7e-3bf4-4fe1-9122-d0667fe59c05",
   "metadata": {},
   "source": [
    "After running the following snippet we have a separate `user.json` for each user.\n",
    "\n",
    "```python\n",
    "split_json_into_subfiles(USER_FILE, USER_DIR)\n",
    "```\n",
    "\n",
    "We can also verify that we now have 1.000.000 user-specific directories in `USER_DIR`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f23232-0e1b-4931-9f87-0f8d7569173e",
   "metadata": {},
   "source": [
    "## Step 3: Extract tweet times\n",
    "Our initial plan was to use this dataset with temporal patterns.\n",
    "As a result, it would be helpful to have the times of tweets readily available.\n",
    "\n",
    "Ultimately, we decided against using the temporal patterns. However, the files created in the following loop are still relevant for the data exploration in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eebf4756-767c-4f83-839b-3af0a101398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_tweet_times(user_dir):\n",
    "    for user in tqdm(user_dir.glob(\"*\")):\n",
    "        if not (user / \"tweets.json\").is_file(): continue\n",
    "        tweet_times = defaultdict(list)\n",
    "        with open(user / \"tweets.json\", \"r\") as fh:\n",
    "            user_tweets = json.load(fh)\n",
    "        for tweet in user_tweets:\n",
    "            tweet_times[\"id\"].append(tweet[\"id\"])\n",
    "            tweet_times[\"created_at\"].append(tweet[\"created_at\"])\n",
    "        tweet_csv = user / \"tweet_times.csv\"\n",
    "        pd.DataFrame(tweet_times, columns=[\"id\", \"created_at\"]).to_csv(tweet_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a05793-753f-49b4-85d2-5a5ade6daa3a",
   "metadata": {},
   "source": [
    "Again, the following snippet takes a long time to finish and only need to be run once.\n",
    "\n",
    "```python\n",
    "    export_tweet_times(USER_DIR)\n",
    "```\n",
    "\n",
    "For example, for user `u1436559360843517957` the resulting table looks like this:\n",
    "\n",
    "| id                   | created_at                |\n",
    "|:---------------------|:--------------------------|\n",
    "| t1487173555120918530 | 2022-01-28 21:19:40+00:00 |\n",
    "| t1485644387140911104 | 2022-01-24 16:03:18+00:00 |\n",
    "| t1464373636530376708 | 2021-11-26 23:20:56+00:00 |\n",
    "| t1447239553216094211 | 2021-10-10 16:36:12+00:00 |\n",
    "| t1447239443111464968 | 2021-10-10 16:35:46+00:00 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7c5cbf-8c71-4308-8f1d-53e983b05aff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 4: Split `edge.csv` into one file per relation\n",
    "\n",
    "In this step the large `edge.csv` file will be split into smaller files based on the type of relation.\n",
    "\n",
    "This is not strictly necessary as we could also load the given `edge.csv` and filter the rows whenever we need to do that.\n",
    "But it reduces the runtime for further processing and gives an better overview of the kind of relations we have.\n",
    "This is especially relevant once we start to add new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b597eb77-a7d9-4884-96bd-b1f16981d6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGE_FILE = DATA_DIR / 'original' / 'edge.csv'\n",
    "RELATION_DIR = DATA_DIR / 'processed' / 'relations'\n",
    "RELATION_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a239f383-db9e-4ae2-9b03-0c761f6a2392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_subfiles(csv_file, category, out_path):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    categories = df[category].unique()\n",
    "    for cat in (pbar := tqdm(categories)):\n",
    "        pbar.set_description(f\"Processing {cat}\")\n",
    "        \n",
    "        category_df = df.loc[df[category] == cat]\n",
    "        category_df.to_csv(out_path / f\"{cat}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a545552d-3bbb-4f5e-8a01-01db7694ae95",
   "metadata": {},
   "source": [
    "The snippet can be used:\n",
    "```python\n",
    "split_into_subfiles(csv_file=EDGE_FILE, category='relation', out_path=RELATION_DIR)\n",
    "```\n",
    "\n",
    "This will create 14 new files, one for each type of relation:\n",
    "```\n",
    "contain.csv      # list  -> tweet\n",
    "discuss.csv      # tweet -> hashtag\n",
    "followed.csv     # list  -> user\n",
    "followers.csv    # user  -> user\n",
    "following.csv    # user  -> user\n",
    "like.csv         # user  -> tweet\n",
    "membership.csv   # list  -> user\n",
    "mentioned.csv    # tweet -> user\n",
    "own.csv          # user  -> list\n",
    "pinned.csv       # user  -> tweet\n",
    "post.csv         # user  -> tweet\n",
    "quoted.csv       # tweet -> tweet\n",
    "replied_to.csv   # tweet -> tweet\n",
    "retweeted.csv    # tweet -> tweet\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067940ee-f3c5-4ecf-927b-f8b46939e067",
   "metadata": {
    "tags": []
   },
   "source": [
    "# End Result\n",
    "\n",
    "This concludes our first round of pre-processing. We split our data into a more granular pieces and up with the following structure:\n",
    "\n",
    "```\n",
    "TwiBot22-data\n",
    " ├── original\n",
    " │   ├── edge.csv\n",
    " │   ├── ...\n",
    " │   └── user.json\n",
    " ├── relations\n",
    " │   ├── contain.csv\n",
    " │   ├── discuss.csv\n",
    " │   ├── followed.csv \n",
    " │   ├── followers.csv \n",
    " │   ├── following.csv \n",
    " │   ├── like.csv \n",
    " │   ├── membership.csv\n",
    " │   ├── mentioned.csv \n",
    " │   ├── own.csv\n",
    " │   ├── pinned.csv  \n",
    " │   ├── post.csv\n",
    " │   ├── quoted.csv    \n",
    " │   ├── replied_to.csv   \n",
    " │   └── retweeted.csv   \n",
    " └── processed\n",
    "     └── users\n",
    "         ├── u1000\n",
    "         │   ├── tweets.json \n",
    "         │   ├── tweet_times.json \n",
    "         │   └── user.csv \n",
    "         ├── u1000001683005542402\n",
    "         │   ├── tweets.json \n",
    "         │   ├── tweet_times.json \n",
    "         │   └── user.csv\n",
    "         ├── ...\n",
    "         └── u999996798793109506\n",
    "             ├── tweets.json \n",
    "             ├── tweet_times.json \n",
    "             └── user.csv\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
